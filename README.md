Linear regression is a commonly used machine learning algorithm that involves
finding a linear relationship between a dependent variable and one or more independent variables.



In order to optimize the model's performance and find the best fit, 
various optimization techniques can be used.
In this project, we will explore my implementation, line by line, of different optimization algorithms
and how different optimization techniques can be applied to a linear regression problem
and visualize the change of the loss and parameters' values with iterations.
Here are some common optimization algorithms that you will find in the project:



1- Batch Gradient Descent

2- Stochastic Gradient Descent 

3- Mini-Batch Gradient Descent

4- Momentum based Gradient Descent

5- Nesterov Accelerated Gradient (NAG)

6- Adaptive Gradient Algorithm (Adagrad)

7- Root Mean Squared Propagation (RMSProp)

8- Adam

9- Broyden–Fletcher–Goldfarb–Shanno (BFGS)
